{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "38898c26-c200-496e-8254-55a00139fbe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax.numpy as jnp\n",
    "from jax import grad, jit, vmap\n",
    "from jax import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "610244c9-9346-4588-96fe-97853d04b427",
   "metadata": {},
   "source": [
    "## Multiplying Matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e02b890e-b370-4587-8616-1107327a0008",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.3721109   0.26423115 -0.18252768 -0.7368197  -0.44030377 -0.1521442\n",
      " -0.67135346 -0.5908641   0.73168886  0.5673026 ]\n"
     ]
    }
   ],
   "source": [
    "key = random.PRNGKey(0)\n",
    "x = random.normal(key, (10,))\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0342645f-e800-4a6f-8241-8877f5535011",
   "metadata": {},
   "source": [
    "我们使用block_until_ready是因为JAX默认是[异步执行](https://blog.csdn.net/m0_63003326/article/details/125813341)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0ac12f3d-690f-46fd-88ba-8db0beb4c116",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27.9 ms ± 2.26 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "size = 3000\n",
    "x = random.normal(key, (size, size), dtype=jnp.float32)\n",
    "%timeit jnp.dot(x, x.T).block_until_ready()  # runs on the GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74563c10-cbed-4370-9c48-76c2265545ac",
   "metadata": {},
   "source": [
    "JAX的numpy函数可以在一般的Numpy数组上工作。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "380198e9-a3dc-4b7a-a9da-4c1ab7d78a1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50.7 ms ± 1.89 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "x = np.random.normal(size=(size, size)).astype(np.float32)\n",
    "%timeit jnp.dot(x, x.T).block_until_ready()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "222a6077-326b-40bf-b320-7f17b9ce3909",
   "metadata": {},
   "source": [
    "比之前慢是因为它每次都将数据转移到GPU上。你可以确保一个NDArray是在设备内存中，使用device_put()。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aa0410ff-1e2e-4080-ab3b-f89eaa299e8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27.3 ms ± 327 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "from jax import device_put\n",
    "\n",
    "x = np.random.normal(size=(size, size)).astype(np.float32)\n",
    "x = device_put(x)\n",
    "%timeit jnp.dot(x, x.T).block_until_ready()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5151df8a-193d-4740-bc8e-fcb2d79edaf0",
   "metadata": {},
   "source": [
    "device_output()的输入仍然是一个NDArray，但是只有当其被使用时才会将数值复制回CPU，其等价于函数jit(lambda x: x)，但是更快。\n",
    "\n",
    "Jax不只是一个GPU后端的Numpy，它也存在很多写数值计算代码很有用的特征。下面是主要的部分：\n",
    "+ jit()，加速你的代码。\n",
    "+ grad()，求导数\n",
    "+ vmap()，自动向量化"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "145b9a9b-1f6e-4144-bab5-8131aaffec1e",
   "metadata": {},
   "source": [
    "## Using jit() to speed up functions\n",
    "如果我们有一系列操作，我们可以使用`@jit`装饰器来一起使用XLA(加速线性代数)来编译。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "93b33944-e7a6-42a6-b263-0b07b79c5fc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "861 µs ± 385 µs per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "def selu(x, alpha=1.67, lmbda=1.05):\n",
    "    return lmbda * jnp.where(x > 0, x, alpha * jnp.exp(x) - alpha)\n",
    "\n",
    "x = random.normal(key, (1000000, ))\n",
    "%timeit selu(x).block_until_ready()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c198057a-0003-4e23-915c-5e1153587b30",
   "metadata": {},
   "source": [
    "我们可以使用`@jit`来加速，它将会在`selu`被一次调用的时候编译之后存储起来。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5e168873-5bc6-48a9-ba98-c5e85223adb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "142 µs ± 4.72 µs per loop (mean ± std. dev. of 7 runs, 10000 loops each)\n"
     ]
    }
   ],
   "source": [
    "selu_jit = jit(selu)\n",
    "%timeit selu_jit(x).block_until_ready()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32f0a550-c665-4b5f-9f7a-48a63ea4d127",
   "metadata": {},
   "source": [
    "## Taking derivatives with grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2bcae02d-b80f-4686-9eea-26855832c1b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.25       0.19661194 0.10499357]\n"
     ]
    }
   ],
   "source": [
    "def sum_logistic(x):\n",
    "    return jnp.sum(1.0 / (1.0 + jnp.exp(-x)))\n",
    "x_small = jnp.arange(3.)\n",
    "derivative_fn = grad(sum_logistic)\n",
    "print(derivative_fn(x_small))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98261fd2-a7f5-4dc4-801e-1ccb3ae7cdbd",
   "metadata": {},
   "source": [
    "检验一下我们的结果是否正确"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "22c5cf0f-56ab-4721-906e-223cef6fd285",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.24998187 0.1965761  0.10502338]\n"
     ]
    }
   ],
   "source": [
    "def first_finite_difference(f, x):\n",
    "    eps = 1e-3\n",
    "    return jnp.array([(f(x + eps * v) - f(x - eps * v)) / (2 * eps) for v in jnp.eye(len(x))])\n",
    "print(first_finite_difference(sum_logistic, x_small))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ebcf969-c5a1-4e29-9886-10b925012e40",
   "metadata": {},
   "source": [
    "我们可以很容易调用`grad().grad()`和`jit()`，可以被任意混合。我们可以："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "76f6fe17-7090-4c98-aa28-c0bccc9665f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray(-0.0353256, dtype=float32, weak_type=True)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grad(jit(grad(jit(grad(sum_logistic)))))(1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c00a21c7-e6a2-4df2-b456-f9cca2c69bb9",
   "metadata": {},
   "source": [
    "对于更高级的自动微分，我们可以使用`jax.vjp()`来进行reverse-mode向量雅克比相乘和`jax.jvp()`对于forward-mode 雅克比向量相乘。这两种操作可以互相或者与其它JAX变换结合。下面是一个高效计算海森矩阵的方式："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "43039c1a-a281-4990-a64f-dfda00633bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax import jacfwd, jacrev\n",
    "def hessian(fun):\n",
    "    return jit(jacfwd(jacrev(fun)))\n",
    "# 这里jacfwd和jacrrev都会返回雅克比矩阵，两种方式返回结果相同，只不过自动微分实现机制不同"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "c826a3aa-dc33-4da0-96fb-125c17763751",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    return jnp.asarray(\n",
    "        [x[0], 5*x[2], 4*x[1]**2 - 2*x[2], x[2] * jnp.sin(x[0])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "47c61339-7aee-41cd-baae-a63da39e3870",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray([[[ 0.       ,  0.       ,  0.       ],\n",
       "              [ 0.       ,  0.       ,  0.       ],\n",
       "              [ 0.       ,  0.       ,  0.       ]],\n",
       "\n",
       "             [[ 0.       ,  0.       ,  0.       ],\n",
       "              [ 0.       ,  0.       ,  0.       ],\n",
       "              [ 0.       ,  0.       ,  0.       ]],\n",
       "\n",
       "             [[ 0.       ,  0.       ,  0.       ],\n",
       "              [ 0.       ,  8.       ,  0.       ],\n",
       "              [ 0.       ,  0.       ,  0.       ]],\n",
       "\n",
       "             [[-2.524413 ,  0.       ,  0.5403023],\n",
       "              [ 0.       ,  0.       ,  0.       ],\n",
       "              [ 0.5403023,  0.       ,  0.       ]]], dtype=float32)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hessian(f)(jnp.array([1.0, 2.0, 3.0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "601971df-dc18-490e-9df1-2c59dae51ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 关于jax.vjp，grad()被认为是vjp()的特殊情况\n",
    "import jax\n",
    "def f(x, y):\n",
    "    return jax.numpy.sin(x), jax.numpy.cos(y)\n",
    "primals, f_vjp = jax.vjp(f, 0.5, 1.0)\n",
    "xbar, ybar = f_vjp((0.5, 1.0))\n",
    "# 比较底层，没看懂，可能得明白自动微分原理"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a8d45a6-c3b1-4f31-8d12-8e1a3a265318",
   "metadata": {},
   "source": [
    "## Auto-vectorization with vmap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "3d10824a-5cdc-48cf-8e89-d6523bbb2358",
   "metadata": {},
   "outputs": [],
   "source": [
    "mat = random.normal(key, (150, 100))\n",
    "batched_x = random.normal(key, (10, 100))\n",
    "\n",
    "def apply_matrix(v):\n",
    "    return jnp.dot(mat, v)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4619074b-13eb-4e89-b10a-dc2d0e8576a5",
   "metadata": {},
   "source": [
    "我们可以用循环的方式将batched_x中的每一个元素与mat相乘"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "b2592b46-32d1-45d3-a3c6-16aa5941384b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.54 ms ± 294 µs per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "def naively_batched_apply_matrix(v_batched):\n",
    "    return jnp.stack([apply_matrix(v) for v in v_batched])\n",
    "%timeit naively_batched_apply_matrix(batched_x).block_until_ready()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c83e1019-ea6f-4777-8784-ec2422186e0c",
   "metadata": {},
   "source": [
    "我们可以让这种操作自动进行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "e31ba940-960c-4690-9778-51595ecd58bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26.5 µs ± 1.44 µs per loop (mean ± std. dev. of 7 runs, 10000 loops each)\n"
     ]
    }
   ],
   "source": [
    "@jit\n",
    "def batched_apply_matrix(v_batched):\n",
    "    return jnp.dot(v_batched, mat.T)\n",
    "%timeit batched_apply_matrix(batched_x).block_until_ready()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f84f6f94-d5e5-4df4-aeac-7c9f4e187af0",
   "metadata": {},
   "source": [
    "可以使用vmap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "c55494ab-417e-4915-9be9-6e63ab0bc49b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32.6 µs ± 4.39 µs per loop (mean ± std. dev. of 7 runs, 10000 loops each)\n"
     ]
    }
   ],
   "source": [
    "@jit\n",
    "def vmap_batched_apply_matrix(v_batched):\n",
    "    return vmap(apply_matrix)(v_batched)\n",
    "%timeit vmap_batched_apply_matrix(batched_x).block_until_ready()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b921059b-4bc0-462e-b458-2ecc6150a805",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
